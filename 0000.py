# -*- coding: utf-8 -*-
"""0000.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Ae01A85HWy7sSsgw1JDELAwzz8MqPac
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install librosa
!pip install resampy
!pip install --upgrade librosa
!pip install streamlit

import librosa
import resampy

# Import necessary libraries
import os
import librosa.display
import numpy as np
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn import metrics
from scipy.io import wavfile as wav
import IPython.display as ipd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation
from tensorflow.keras.callbacks import ModelCheckpoint
from datetime import datetime
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split

filname='/content/drive/MyDrive/seeem/files/fold1/T_1000001.wav'

from scipy.io import wavfile as wav
wave_sample_rate, wav_audio = wav.read(filname)

from google.colab import drive
drive.mount('/content/drive')

wave_sample_rate

wav_audio

from google.colab import drive
drive.mount('/content/drive/MyDrive/seeem/Metadata.xlsx')

import pandas as pd
metadata=pd.read_excel('/content/drive/MyDrive/seeem/Metadata.xlsx')
metadata.head()

metadata['Label'].value_counts()

# Sound
audioFilePath= "/content/drive/MyDrive/seeem/files/fold2/T_1001509.wav"
librosa_audio_data, librosa_sample_rate = librosa.load(audioFilePath)

mfccs = librosa.feature.mfcc(y=librosa_audio_data, sr=librosa_sample_rate)
print(mfccs.shape)

# Extracting mfcc for every audio file
import os
audio_dataset_path = '/content/drive/MyDrive/seeem/files'
metadata=pd.read_excel('/content/drive/MyDrive/seeem/Metadata.xlsx')
metadata.head()

def features_extractor(file):
    audio,sample_rate=librosa.load(file_name, res_type='kaiser_fast')
    mfccs_features=librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)
    mfccs_scaled_features=np.mean(mfccs_features.T,axis=0)
    return mfccs_scaled_features

extracted_features=[]
for index_num,row in tqdm(metadata.iterrows()):
    # Verify the correct column name for 'fold' in your DataFrame
    # Replace 'correct_fold_column' with the actual column name if 'fold' is incorrect
    file_name=os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row["fold"])+'/',str(row["file_Name"]))
    final_class_labels=row["Label"]
    data=features_extractor(file_name)
    extracted_features.append([data,final_class_labels])

extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','Label'])
extracted_features_df.head()

### Split the dataset into independant and depandent dataset
X=np.array(extracted_features_df['feature'].tolist())
y=np.array(extracted_features_df['Label'].tolist())

# Label Encoding
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical  # Import to_categorical

labelEncoder = LabelEncoder()

# Ensure y is a 1D array before label encoding
# If y is already a 1D array of labels, no need for argmax
y_encoded = to_categorical(labelEncoder.fit_transform(y))

# Train test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.2, random_state=42)

# Model creation
# Example: Assuming y is a NumPy array with 10 samples and 5 labels
y = np.random.rand(10, 5)

num_labels = y.shape[1]
print(num_labels)  # Output: 5

def features_extractor(file):
    audio, sample_rate = librosa.load(file, res_type='kaiser_fast')
    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)
    mfccs_scaled_features = np.mean(mfccs_features.T, axis=0)
    return mfccs_scaled_features

# Extract features from audio files
extracted_features = []
for index_num, row in tqdm(metadata.iterrows(), total=len(metadata)):
    file_name = os.path.join(audio_dataset_path, 'fold' + str(row["fold"]), str(row["file_Name"]))
    final_class_labels = row["Label"]
    data = features_extractor(file_name)
    extracted_features.append([data, final_class_labels])

# Create DataFrame from extracted features
extracted_features_df = pd.DataFrame(extracted_features, columns=['feature', 'Label'])

# Split dataset into independent and dependent datasets
X = np.array(extracted_features_df['feature'].tolist())
y = np.array(extracted_features_df['Label'].tolist())

# Encode labels
label_encoder = LabelEncoder()
y = to_categorical(label_encoder.fit_transform(y))

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model Definition
model = Sequential()

# Add layers
model.add(Dense(100, input_shape=(40,)))
model.add(Activation('relu'))
model.add(Dropout(0.5))
# 2ns layer
model.add(Dense(100))
model.add(Activation('relu'))
model.add(Dropout(0.5))
#final layer
model.add(Dense(y.shape[1]))  # Output layer matching number of classes
model.add(Activation('softmax'))

# Compile model
model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')

# Model Checkpoint to save the best model
##checkpointer = ModelCheckpoint(filepath='/content/drive/MyDrive/audio_forensic.hdf5',
                     ##          verbose=1, save_best_only=True)
# Model Checkpoint to save the best model
checkpointer = ModelCheckpoint(filepath='/content/drive/MyDrive/audio_forensic.keras', # Change file extension to .keras
                               verbose=1, save_best_only=True)

# Train the model
num_epochs = 100
num_batch_size = 50

start = datetime.now()
model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs,
          validation_data=(X_test, y_test), callbacks=[checkpointer])
duration = datetime.now() - start

print("Training completed in time:", duration)

test_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(test_accuracy[1])

fileName = "/content/drive/MyDrive/FYP - Digital Audio Forensics/COMBINED/fold2/fake1_1.wav"
pred_feature=features_extractor(fileName)
model.predict(pred_feature.reshape(1,-1))
pred_feature.reshape(1,-1).shape
metadata['Label'].unique()

FileName = '/content/drive/MyDrive/FYP - Digital Audio Forensics/COMBINED/fold1/speaker1_1.wav'
audio, sample_rate = librosa.load(FileName, res_type='kaiser_fast')
mfccs_features=librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)
mfccs_scaled_features=np.mean(mfccs_features.T,axis=0)

#print(mfccs_scaled_features)
mfccs_scaled_features=mfccs_scaled_features.reshape(1,-1)
#print(mfccs_scaled_features)
#print(mfccs_scaled_features.shape)
predicted_probabilities=model.predict(mfccs_scaled_features) # Store the probabilities
#print(predicted_probabilities)
predicted_label = np.argmax(predicted_probabilities, axis=1) # Get the class with highest probability
#print(predicted_label)
prediction_class =labelEncoder.inverse_transform(predicted_label) # Now use inverse_transform on 1D array
print(prediction_class)

if prediction_class=='Real':
  print('This is real audio')
else:
  print('This is fake /AI_Generated audio')

